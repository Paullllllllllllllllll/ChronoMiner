# model_config.yaml
# ==================
# LLM model configuration for structured extraction.
# Used by: modules/llm/langchain_provider.py, modules/llm/openai_utils.py
#
# ── Supported Providers ────────────────────────────────────────────────────────
#   openai:     GPT-4o, GPT-4.1, o1, o3, GPT-5      (env: OPENAI_API_KEY)
#   anthropic:  Claude 3.5 Sonnet, Claude 3 Opus    (env: ANTHROPIC_API_KEY)
#   google:     Gemini 2.0 Flash, Gemini 1.5 Pro    (env: GOOGLE_API_KEY)
#   openrouter: Unified access to 200+ models       (env: OPENROUTER_API_KEY)
#
# ── Model Name Examples ────────────────────────────────────────────────────────
#   OpenAI:      gpt-4o, gpt-4.1-mini, o1, o3, gpt-5-mini
#   Anthropic:   claude-3-5-sonnet-20241022, claude-3-opus-20240229
#   Google:      gemini-2.0-flash, gemini-1.5-pro
#   OpenRouter:  anthropic/claude-sonnet-4-5, google/gemini-2.5-flash, deepseek/deepseek-r1

transcription_model:
  # provider: openai           # Optional override; auto-detected from model name if omitted

  name: "gpt-5-mini"           # Model identifier; provider auto-detected from prefix
                               # Examples: "gpt-4o" → OpenAI, "claude-*" → Anthropic

  max_output_tokens: 32000     # Maximum tokens the model can generate per response

  # ── Reasoning Controls ───────────────────────────────────────────────────────
  # For reasoning-capable models (GPT-5, o-series, Claude 4.x, Gemini 2.5+)
  # Effort levels control depth vs speed tradeoff:
  #   low:    Minimal reasoning, fastest responses
  #   medium: Balanced depth (recommended for most tasks)
  #   high:   Maximum depth, slower but more thorough
  reasoning:
    effort: low

  # ── Text Output Controls ─────────────────────────────────────────────────────
  # GPT-5 family specific setting for response verbosity
  text:
    verbosity: high            # low | medium | high

  # ── Sampler Controls ─────────────────────────────────────────────────────────
  # Classic sampling parameters for non-reasoning models (GPT-4o, Claude, Gemini)
  # Automatically ignored for reasoning models by the provider adapter
  temperature: 0.0             # 0.0 = deterministic; higher = more creative
  top_p: 1.0                   # Nucleus sampling threshold
  frequency_penalty: 0.0       # Penalize repeated tokens
  presence_penalty: 0.0        # Penalize tokens already in context
