# =============================================================================
# Concurrency Configuration
# =============================================================================
# Controls parallel processing, API retry behavior, and token usage limits.
#
# Used by:
#   - modules/core/processing_strategy.py (concurrency_limit, delay_between_tasks)
#   - modules/llm/langchain_provider.py (retry configuration)
#   - modules/llm/batch/backends/*.py (batch processing backends)
#   - modules/core/token_tracker.py (daily_token_limit)
# =============================================================================

concurrency:
  # -------------------------------------------------------------------------
  # Text Processing
  # -------------------------------------------------------------------------
  # Settings for line range generation and text preprocessing operations.
  text_processing:
    # Maximum concurrent API calls for text processing tasks.
    concurrency_limit: 1500

    # Delay (seconds) between starting each task.
    delay_between_tasks: 0.05

  # -------------------------------------------------------------------------
  # Extraction
  # -------------------------------------------------------------------------
  # Settings for LLM extraction calls (synchronous and batch modes).
  extraction:
    # Maximum concurrent LLM API calls during synchronous extraction.
    # Higher values = faster processing but more API load.
    # Recommended: 20-50 for most use cases; up to 1500 for high-throughput scenarios.
    # Note: Anthropic is automatically limited to 1 due to rate limits.
    concurrency_limit: 1500

    # Delay (seconds) between starting each extraction task.
    # Helps avoid rate limiting; set to 0 for maximum throughput.
    delay_between_tasks: 0.05

    # OpenAI service tier for synchronous mode.
    # Values: 'auto', 'default', 'flex', 'priority'
    # Note: 'flex' is synchronous-only; batch processing may require 'auto'.
    service_tier: flex

    # Timeout configuration (seconds).
    timeouts:
      # Maximum time for entire request lifecycle.
      total: 900

    # Retry configuration for transient API errors (429, 5xx, timeouts).
    # LangChain handles exponential backoff internally.
    retry:
      # Maximum retry attempts before failing.
      attempts: 150

      # Initial backoff wait time (seconds).
      wait_min_seconds: 15

      # Maximum backoff wait time (exponential backoff capped here).
      wait_max_seconds: 120

      # Random jitter added to prevent thundering herd.
      jitter_max_seconds: 5

# =============================================================================
# Daily Token Limit
# =============================================================================
# Tracks cumulative token usage across all LLM API calls and enforces a daily
# cap. When the limit is reached, processing pauses until midnight reset.
#
# Used by:
#   - modules/core/token_tracker.py (limit enforcement and usage tracking)
#   - modules/operations/extraction/run.py (check before each file)
# =============================================================================

daily_token_limit:
  # Master toggle. When false, no token tracking or limits are enforced.
  enabled: false

  # Maximum tokens allowed per day (input + output combined).
  # Processing pauses when this limit is reached; resumes at midnight.
  daily_tokens: 9000000
