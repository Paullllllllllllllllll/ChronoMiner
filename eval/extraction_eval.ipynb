{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoMiner Extraction Evaluation\n",
    "\n",
    "This notebook evaluates structured data extraction quality across multiple LLM providers and models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure you have:\n",
    "1. Corrected transcription files in `test_data/input/{category}/`\n",
    "2. Ground truth JSON files in `test_data/ground_truth/{category}/`\n",
    "3. Model outputs in `test_data/output/{category}/{model_name}/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Add parent directory for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from metrics import (\n",
    "    ExtractionMetrics,\n",
    "    aggregate_metrics,\n",
    "    compute_extraction_metrics,\n",
    "    format_field_metrics_table,\n",
    "    format_metrics_table,\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation config\n",
    "config_path = Path(\"eval_config.yaml\")\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract paths\n",
    "base_path = Path.cwd()\n",
    "input_path = base_path / config[\"dataset\"][\"input_path\"]\n",
    "output_path = base_path / config[\"dataset\"][\"output_path\"]\n",
    "ground_truth_path = base_path / config[\"dataset\"][\"ground_truth_path\"]\n",
    "reports_path = base_path / config[\"evaluation\"][\"reports_path\"]\n",
    "\n",
    "# Evaluation settings\n",
    "threshold = config[\"evaluation\"].get(\"string_similarity_threshold\", 0.85)\n",
    "case_sensitive = config[\"evaluation\"].get(\"case_sensitive\", False)\n",
    "normalize_ws = config[\"evaluation\"].get(\"normalize_whitespace\", True)\n",
    "schema_fields = config[\"evaluation\"].get(\"schema_fields\", {})\n",
    "\n",
    "categories = config[\"dataset\"][\"categories\"]\n",
    "models = config[\"models\"]\n",
    "\n",
    "print(f\"Categories: {[c['name'] for c in categories]}\")\n",
    "print(f\"Models: {[m['name'] for m in models]}\")\n",
    "print(f\"Similarity threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Available Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_files(category_name: str, model_name: str):\n",
    "    \"\"\"Find matching input, output, and ground truth files.\"\"\"\n",
    "    input_dir = input_path / category_name\n",
    "    output_dir = output_path / category_name / model_name\n",
    "    gt_dir = ground_truth_path / category_name\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        print(f\"Warning: Input directory does not exist: {input_dir}\")\n",
    "        return files\n",
    "    \n",
    "    for input_file in sorted(input_dir.glob(\"*.txt\")):\n",
    "        stem = input_file.stem\n",
    "        if stem.endswith(\"_line_ranges\") or stem.endswith(\"_context\"):\n",
    "            continue\n",
    "        \n",
    "        gt_file = gt_dir / f\"{stem}.json\"\n",
    "        output_file = output_dir / f\"{stem}.json\"\n",
    "        \n",
    "        if gt_file.exists():\n",
    "            files.append({\n",
    "                \"name\": stem,\n",
    "                \"input\": input_file,\n",
    "                \"output\": output_file,\n",
    "                \"ground_truth\": gt_file,\n",
    "                \"has_output\": output_file.exists(),\n",
    "            })\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Show available files for each category\n",
    "for cat in categories:\n",
    "    cat_name = cat[\"name\"]\n",
    "    files = discover_files(cat_name, models[0][\"name\"])\n",
    "    print(f\"\\n{cat_name}: {len(files)} files with ground truth\")\n",
    "    for f in files[:5]:  # Show first 5\n",
    "        status = \"✓\" if f[\"has_output\"] else \"✗\"\n",
    "        print(f\"  {status} {f['name']}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"  ... and {len(files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path) -> dict:\n",
    "    \"\"\"Load JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def evaluate_category(category: dict, model: dict) -> ExtractionMetrics:\n",
    "    \"\"\"Evaluate all files in a category for a model.\"\"\"\n",
    "    cat_name = category[\"name\"]\n",
    "    schema_name = category[\"schema\"]\n",
    "    model_name = model[\"name\"]\n",
    "    \n",
    "    fields = schema_fields.get(schema_name, [])\n",
    "    files = discover_files(cat_name, model_name)\n",
    "    \n",
    "    file_metrics = []\n",
    "    \n",
    "    for f in files:\n",
    "        if not f[\"has_output\"]:\n",
    "            continue\n",
    "        \n",
    "        output_data = load_json(f[\"output\"])\n",
    "        gt_data = load_json(f[\"ground_truth\"])\n",
    "        \n",
    "        if not output_data or not gt_data:\n",
    "            continue\n",
    "        \n",
    "        metrics = compute_extraction_metrics(\n",
    "            ground_truth=gt_data,\n",
    "            hypothesis=output_data,\n",
    "            fields_to_evaluate=fields if fields else None,\n",
    "            threshold=threshold,\n",
    "            case_sensitive=case_sensitive,\n",
    "            normalize_ws=normalize_ws,\n",
    "        )\n",
    "        \n",
    "        file_metrics.append(metrics)\n",
    "    \n",
    "    if file_metrics:\n",
    "        return aggregate_metrics(file_metrics)\n",
    "    return ExtractionMetrics()\n",
    "\n",
    "# Run evaluation\n",
    "all_metrics = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model[\"name\"]\n",
    "    all_metrics[model_name] = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_name = category[\"name\"]\n",
    "        \n",
    "        metrics = evaluate_category(category, model)\n",
    "        \n",
    "        if metrics.total_gt_entries > 0:\n",
    "            all_metrics[model_name][cat_name] = metrics\n",
    "            print(f\"{model_name} / {cat_name}: Entry F1={metrics.entry_f1:.2%}, Micro F1={metrics.micro_f1:.2%}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Display summary table\n",
    "summary_table = format_metrics_table(all_metrics)\n",
    "display(Markdown(summary_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field-Level Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show field-level breakdown for each model/category\n",
    "for model_name, cat_metrics in all_metrics.items():\n",
    "    for cat_name, metrics in cat_metrics.items():\n",
    "        display(Markdown(f\"### {model_name} / {cat_name}\"))\n",
    "        display(Markdown(format_field_metrics_table(metrics)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON\n",
    "json_path = reports_path / f\"eval_results_{timestamp}.json\"\n",
    "json_data = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"config\": {\n",
    "        \"threshold\": threshold,\n",
    "        \"case_sensitive\": case_sensitive,\n",
    "        \"normalize_whitespace\": normalize_ws,\n",
    "    },\n",
    "    \"results\": {\n",
    "        model: {cat: m.to_dict() for cat, m in cats.items()}\n",
    "        for model, cats in all_metrics.items()\n",
    "    },\n",
    "}\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Saved: {json_path}\")\n",
    "\n",
    "# Save CSV\n",
    "csv_path = reports_path / f\"eval_results_{timestamp}.csv\"\n",
    "rows = []\n",
    "for model_name, cat_metrics in all_metrics.items():\n",
    "    for cat_name, m in cat_metrics.items():\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"category\": cat_name,\n",
    "            \"entry_precision\": round(m.entry_precision * 100, 2),\n",
    "            \"entry_recall\": round(m.entry_recall * 100, 2),\n",
    "            \"entry_f1\": round(m.entry_f1 * 100, 2),\n",
    "            \"micro_precision\": round(m.micro_precision * 100, 2),\n",
    "            \"micro_recall\": round(m.micro_recall * 100, 2),\n",
    "            \"micro_f1\": round(m.micro_f1 * 100, 2),\n",
    "        })\n",
    "if rows:\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "\n",
    "# Save Markdown\n",
    "md_path = reports_path / f\"eval_results_{timestamp}.md\"\n",
    "md_content = f\"\"\"# ChronoMiner Extraction Evaluation Results\n",
    "\n",
    "**Generated:** {timestamp}\n",
    "\n",
    "## Summary\n",
    "\n",
    "{format_metrics_table(all_metrics)}\n",
    "\"\"\"\n",
    "with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md_content)\n",
    "print(f\"Saved: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    model_names = list(all_metrics.keys())\n",
    "    cat_names = list(set(\n",
    "        cat for cats in all_metrics.values() for cat in cats.keys()\n",
    "    ))\n",
    "    \n",
    "    # Create grouped bar chart for F1 scores\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.8 / len(cat_names)\n",
    "    \n",
    "    for i, cat in enumerate(cat_names):\n",
    "        f1_scores = [\n",
    "            all_metrics[model].get(cat, ExtractionMetrics()).micro_f1 * 100\n",
    "            for model in model_names\n",
    "        ]\n",
    "        offset = (i - len(cat_names) / 2 + 0.5) * width\n",
    "        ax.bar(x + offset, f1_scores, width, label=cat)\n",
    "    \n",
    "    ax.set_ylabel(\"Micro F1 Score (%)\")\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_title(\"Extraction Quality by Model and Category\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "    ax.legend(title=\"Category\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save chart\n",
    "    chart_path = reports_path / f\"eval_chart_{timestamp}.png\"\n",
    "    plt.savefig(chart_path, dpi=150)\n",
    "    print(f\"Saved: {chart_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"matplotlib not available - skipping visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
