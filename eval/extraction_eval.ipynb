{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoMiner Extraction Evaluation\n",
    "\n",
    "This notebook evaluates the quality of structured data extraction produced by different LLM models on the ChronoMiner evaluation set. The unit of evaluation is a document chunk: each model output chunk is compared against a corresponding ground-truth chunk, and metrics are aggregated across chunks, sources, categories, and models.\n",
    "\n",
    "## Evaluation method (chunk-level comparison)\n",
    "\n",
    "The evaluation operates on the temporary JSONL outputs produced by the extraction pipeline. For each (category, source, model) combination, the notebook:\n",
    "\n",
    "- Loads ground-truth chunk extractions from test_data/ground_truth/{category}/.\n",
    "- Loads model output chunk extractions from test_data/output/{category}/{model_name}/.\n",
    "- Aligns chunks and computes metrics per aligned chunk using compute_extraction_metrics().\n",
    "- Aggregates metrics across chunks using aggregate_metrics().\n",
    "\n",
    "This design:\n",
    "- Avoids penalizing irrelevant formatting differences in serialized JSON.\n",
    "- Preserves per-chunk error attribution for debugging and model comparison.\n",
    "- Keeps evaluation logic consistent across heterogeneous document types.\n",
    "\n",
    "## Reproducibility and paper-ready outputs\n",
    "\n",
    "This notebook is intended to support academic reporting and reproducible evaluation:\n",
    "\n",
    "- Results tables are rendered as HTML in the notebook for inspection.\n",
    "- If SAVE_TABLES_LATEX is set to True, the same tables are exported as LaTeX (.tex) files into OUTPUT_DIR for direct inclusion in manuscripts.\n",
    "- Machine-readable exports (JSON and CSV) are written to reports_path as configured in eval_config.yaml.\n",
    "\n",
    "## Required inputs (expected directory layout)\n",
    "\n",
    "Before running the notebook, ensure the evaluation data are present:\n",
    "\n",
    "1. Input sources (used to discover which documents exist):\n",
    "   test_data/input/{category}/*.txt\n",
    "\n",
    "2. Ground truth chunk extractions (JSONL preferred):\n",
    "   test_data/ground_truth/{category}/*.jsonl\n",
    "\n",
    "3. Model output chunk extractions (JSONL):\n",
    "   test_data/output/{category}/{model_name}/**/*.jsonl"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. **Load Configuration**\n",
    "   Load eval_config.yaml, resolve dataset/report paths, and set evaluation parameters (e.g., string similarity threshold).\n",
    "\n",
    "2. **Discover Available Data**\n",
    "   Inspect test_data/ to list available sources, available model outputs, and ground-truth coverage by category.\n",
    "\n",
    "3. **Chunk-Level Evaluation**\n",
    "   Define the evaluation data structures and the chunk-alignment logic used to compare predicted vs. ground-truth extractions.\n",
    "\n",
    "4. **Run Evaluation**\n",
    "   Execute the evaluation across all configured models and categories, aggregate chunk-level metrics, and store results for reporting.\n",
    "\n",
    "5. **Results Summary Table**\n",
    "   Produce a publication-ready summary table of entry-level and micro-averaged metrics by model and category; display as HTML and optionally export as LaTeX.\n",
    "\n",
    "6. **Field-Level Breakdown**\n",
    "   Report field-wise precision/recall/F1 and error counts (TP/FP/FN) for each model/category; display as HTML and optionally export as LaTeX.\n",
    "\n",
    "7. **Per-Source Details (Optional)**\n",
    "   Provide an optional snippet to inspect per-source performance for a selected model and category.\n",
    "\n",
    "8. **Save Reports**\n",
    "   Export machine-readable results as JSON and CSV (and LaTeX tables if enabled) into the configured reports directories.\n",
    "\n",
    "9. **Visualization (Optional)**\n",
    "   Create a grouped bar chart of micro F1 by model and category; save as PNG and optionally export a PDF for LaTeX inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Imports and Global Configuration\n",
    "# ================================================================\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add parent directory for imports\n",
    "EVAL_DIR = Path.cwd()\n",
    "PROJECT_ROOT = EVAL_DIR.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "\n",
    "# Import extraction metrics\n",
    "from metrics import (\n",
    "    ExtractionMetrics,\n",
    "    aggregate_metrics,\n",
    "    compute_extraction_metrics,\n",
    ")\n",
    "\n",
    "# Import JSONL chunk-level utilities\n",
    "from jsonl_eval import (\n",
    "    ChunkExtraction,\n",
    "    DocumentExtractions,\n",
    "    parse_extraction_jsonl,\n",
    "    find_jsonl_file,\n",
    "    load_chunk_extractions,\n",
    "    load_ground_truth_chunks,\n",
    "    align_chunks,\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# OUTPUT CONFIGURATION\n",
    "# ================================================================\n",
    "# Set to True to save all result tables as LaTeX (.tex) files\n",
    "SAVE_TABLES_LATEX = True\n",
    "\n",
    "# Directory where LaTeX tables will be saved (relative to EVAL_DIR)\n",
    "OUTPUT_DIR = EVAL_DIR / \"reports\" / \"tables\"\n",
    "\n",
    "# ================================================================\n",
    "# Display Configuration\n",
    "# ================================================================\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Create output directory if saving LaTeX\n",
    "if SAVE_TABLES_LATEX:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"Analysis run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Save tables as LaTeX: {SAVE_TABLES_LATEX}\")\n",
    "if SAVE_TABLES_LATEX:\n",
    "    print(f\"LaTeX output directory: {OUTPUT_DIR.resolve()}\")\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "This section loads the evaluation configuration from eval_config.yaml and materializes all key runtime parameters used throughout the notebook.\n",
    "\n",
    "Specifically, the configuration determines:\n",
    "\n",
    "- Dataset paths\n",
    "  - input_path: where input .txt sources are located (by category)\n",
    "  - output_path: where model-produced JSONL extractions are located\n",
    "  - ground_truth_path: where ground-truth JSONL extractions are located\n",
    "  - reports_path: where exported reports (JSON/CSV/figures) should be written\n",
    "\n",
    "- Evaluation parameters\n",
    "  - threshold: string similarity threshold used inside compute_extraction_metrics()\n",
    "  - case_sensitive and normalize_whitespace: normalization controls applied to string comparison\n",
    "  - schema_fields: per-schema field selection used to restrict evaluation to a subset of fields\n",
    "\n",
    "The code cell below prints a short configuration summary so each run leaves an auditable record of paths and parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Load Evaluation Configuration\n",
    "# ================================================================\n",
    "\n",
    "config_path = EVAL_DIR / \"eval_config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract paths\n",
    "input_path = EVAL_DIR / config[\"dataset\"][\"input_path\"]\n",
    "output_path = EVAL_DIR / config[\"dataset\"][\"output_path\"]\n",
    "ground_truth_path = EVAL_DIR / config[\"dataset\"][\"ground_truth_path\"]\n",
    "reports_path = EVAL_DIR / config[\"evaluation\"][\"reports_path\"]\n",
    "\n",
    "# Evaluation settings\n",
    "threshold = config[\"evaluation\"].get(\"string_similarity_threshold\", 0.85)\n",
    "case_sensitive = config[\"evaluation\"].get(\"case_sensitive\", False)\n",
    "normalize_ws = config[\"evaluation\"].get(\"normalize_whitespace\", True)\n",
    "schema_fields = config[\"evaluation\"].get(\"schema_fields\", {})\n",
    "\n",
    "categories = config[\"dataset\"][\"categories\"]\n",
    "models = config[\"models\"]\n",
    "\n",
    "# Create reports directory\n",
    "reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Display configuration summary\n",
    "print(f\"Categories: {[c['name'] for c in categories]}\")\n",
    "print(f\"Models: {[m['name'] for m in models]}\")\n",
    "print(f\"Similarity threshold: {threshold}\")\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Input: {input_path}\")\n",
    "print(f\"  Output: {output_path}\")\n",
    "print(f\"  Ground Truth: {ground_truth_path}\")\n",
    "print(f\"  Reports: {reports_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Available Data\n",
    "\n",
    "This section performs a lightweight inventory of the evaluation dataset and outputs to help you validate that all expected inputs are present before running the full evaluation.\n",
    "\n",
    "For each configured category, it reports:\n",
    "\n",
    "- Input sources: discovered from input_path / {category} by scanning *.txt files (excluding helper files such as _line_ranges or _context).\n",
    "- Available model outputs: discovered under output_path / {category} by checking which model subdirectories contain JSONL extraction outputs.\n",
    "- Ground truth availability: checked under ground_truth_path / {category} (JSONL preferred, with optional legacy JSON fallback).\n",
    "\n",
    "The printed summary is primarily a diagnostic aid:\n",
    "- If a category has no ground truth, it cannot be evaluated.\n",
    "- If a model has no outputs for a category, it will be skipped for that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_sources(category_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover source files in the input directory for a category.\n",
    "    \n",
    "    Returns:\n",
    "        List of source names (without extension)\n",
    "    \"\"\"\n",
    "    input_dir = input_path / category_name\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    sources = []\n",
    "    for input_file in sorted(input_dir.glob(\"*.txt\")):\n",
    "        stem = input_file.stem\n",
    "        if stem.endswith(\"_line_ranges\") or stem.endswith(\"_context\"):\n",
    "            continue\n",
    "        sources.append(stem)\n",
    "    \n",
    "    return sources\n",
    "\n",
    "\n",
    "def discover_available_models(category_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover which models have JSONL output for a category.\n",
    "    \n",
    "    Returns:\n",
    "        List of model names with available output\n",
    "    \"\"\"\n",
    "    cat_output = output_path / category_name\n",
    "    \n",
    "    if not cat_output.exists():\n",
    "        return []\n",
    "    \n",
    "    available = []\n",
    "    for d in cat_output.iterdir():\n",
    "        if d.is_dir():\n",
    "            # Check if model directory has any JSONL files\n",
    "            jsonl_files = list(d.rglob(\"*.jsonl\"))\n",
    "            # Filter out batch tracking files\n",
    "            jsonl_files = [f for f in jsonl_files if \"_batch_\" not in f.name]\n",
    "            if jsonl_files:\n",
    "                available.append(d.name)\n",
    "    \n",
    "    return sorted(available)\n",
    "\n",
    "\n",
    "def check_ground_truth_available(category_name: str) -> Tuple[bool, int, str]:\n",
    "    \"\"\"\n",
    "    Check if ground truth files exist for a category.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (has_ground_truth, count_of_files, format)\n",
    "    \"\"\"\n",
    "    gt_dir = ground_truth_path / category_name\n",
    "    if not gt_dir.exists():\n",
    "        return False, 0, \"none\"\n",
    "    \n",
    "    # Check for JSONL format (preferred)\n",
    "    jsonl_files = list(gt_dir.glob(\"*.jsonl\"))\n",
    "    if jsonl_files:\n",
    "        return True, len(jsonl_files), \"jsonl\"\n",
    "    \n",
    "    # Fall back to JSON format (legacy)\n",
    "    json_files = list(gt_dir.glob(\"*.json\"))\n",
    "    if json_files:\n",
    "        return True, len(json_files), \"json\"\n",
    "    \n",
    "    return False, 0, \"none\"\n",
    "\n",
    "\n",
    "# Discover and display available data\n",
    "print(\"=\" * 60)\n",
    "print(\"AVAILABLE DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_summary = {}\n",
    "\n",
    "for cat in categories:\n",
    "    cat_name = cat[\"name\"]\n",
    "    sources = discover_sources(cat_name)\n",
    "    available_models = discover_available_models(cat_name)\n",
    "    gt_available, gt_count, gt_format = check_ground_truth_available(cat_name)\n",
    "    \n",
    "    data_summary[cat_name] = {\n",
    "        \"sources\": sources,\n",
    "        \"models\": available_models,\n",
    "        \"ground_truth_available\": gt_available,\n",
    "        \"ground_truth_count\": gt_count,\n",
    "        \"ground_truth_format\": gt_format,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{cat_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Input sources: {len(sources)}\")\n",
    "    if sources:\n",
    "        for s in sources[:5]:\n",
    "            print(f\"    - {s}\")\n",
    "        if len(sources) > 5:\n",
    "            print(f\"    ... and {len(sources) - 5} more\")\n",
    "    print(f\"  Models with JSONL output: {len(available_models)}\")\n",
    "    for m in available_models:\n",
    "        print(f\"    - {m}\")\n",
    "    print(f\"  Ground truth: {'Yes' if gt_available else 'No'} ({gt_count} files, format: {gt_format})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk-Level Evaluation\n",
    "\n",
    "This section defines the chunk-level evaluation logic used throughout the notebook.\n",
    "\n",
    "### Unit of analysis\n",
    "\n",
    "- A chunk corresponds to one JSONL extraction record produced by the extraction pipeline for a contiguous portion of a source document.\n",
    "- Each chunk can contain zero or more extracted entries.\n",
    "- Metrics are computed per chunk and then aggregated.\n",
    "\n",
    "### Core data structures and logic\n",
    "\n",
    "The code cell below defines:\n",
    "\n",
    "- ChunkEvaluationResult: stores per-chunk evaluation status and metrics (including failure modes such as missing ground truth or missing model output).\n",
    "- SourceEvaluationResult: stores all chunk results for one (category, model, source) combination plus an aggregated metric summary.\n",
    "- evaluate_source_chunks(...): loads ground truth and model outputs, aligns chunks, computes chunk metrics via compute_extraction_metrics(), and aggregates them via aggregate_metrics().\n",
    "\n",
    "### Methodological notes\n",
    "\n",
    "- Field selection: the evaluated fields are drawn from schema_fields[schema_name] when available. This supports schema-specific evaluation (e.g., bibliography vs. address books).\n",
    "- Aggregation: aggregated results summarize performance across all valid chunks for a source, and later across all sources within a category/model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkEvaluationResult:\n",
    "    \"\"\"Container for per-chunk evaluation results.\"\"\"\n",
    "    chunk_index: int\n",
    "    custom_id: str\n",
    "    metrics: Optional[ExtractionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    gt_entry_count: int = 0\n",
    "    hyp_entry_count: int = 0\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SourceEvaluationResult:\n",
    "    \"\"\"Container for source-level evaluation results.\"\"\"\n",
    "    category: str\n",
    "    model_name: str\n",
    "    source_name: str\n",
    "    chunk_results: List[ChunkEvaluationResult]\n",
    "    aggregated_metrics: Optional[ExtractionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def total_chunks(self) -> int:\n",
    "        return len(self.chunk_results)\n",
    "    \n",
    "    @property\n",
    "    def evaluated_chunks(self) -> int:\n",
    "        return sum(1 for c in self.chunk_results if c.metrics is not None)\n",
    "\n",
    "\n",
    "def evaluate_source_chunks(\n",
    "    category_name: str,\n",
    "    model_name: str,\n",
    "    source_name: str,\n",
    "    schema_name: str,\n",
    ") -> SourceEvaluationResult:\n",
    "    \"\"\"\n",
    "    Evaluate a source by comparing chunks from model output to ground truth.\n",
    "    \n",
    "    Args:\n",
    "        category_name: Dataset category\n",
    "        model_name: Model identifier\n",
    "        source_name: Source file name\n",
    "        schema_name: Schema name for field selection\n",
    "        \n",
    "    Returns:\n",
    "        SourceEvaluationResult with per-chunk and aggregated metrics\n",
    "    \"\"\"\n",
    "    # Load ground truth chunks\n",
    "    gt_doc = load_ground_truth_chunks(ground_truth_path, category_name, source_name)\n",
    "    if gt_doc is None or not gt_doc.chunks:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category_name,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            chunk_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=False,\n",
    "            output_found=False,\n",
    "            error=\"Ground truth not found\",\n",
    "        )\n",
    "    \n",
    "    # Load model output chunks\n",
    "    hyp_doc = load_chunk_extractions(output_path, category_name, model_name, source_name)\n",
    "    if hyp_doc is None or not hyp_doc.chunks:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category_name,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            chunk_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=True,\n",
    "            output_found=False,\n",
    "            error=\"Model output not found\",\n",
    "        )\n",
    "    \n",
    "    # Get fields to evaluate for this schema\n",
    "    fields = schema_fields.get(schema_name, [])\n",
    "    \n",
    "    # Align chunks\n",
    "    aligned = align_chunks(hyp_doc, gt_doc)\n",
    "    \n",
    "    # Compute per-chunk metrics\n",
    "    chunk_results: List[ChunkEvaluationResult] = []\n",
    "    valid_metrics: List[ExtractionMetrics] = []\n",
    "    \n",
    "    for hyp_chunk, gt_chunk in aligned:\n",
    "        # Determine chunk info\n",
    "        if gt_chunk:\n",
    "            chunk_index = gt_chunk.chunk_index\n",
    "            custom_id = gt_chunk.custom_id or (hyp_chunk.custom_id if hyp_chunk else \"\")\n",
    "        elif hyp_chunk:\n",
    "            chunk_index = hyp_chunk.chunk_index\n",
    "            custom_id = hyp_chunk.custom_id\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Check availability\n",
    "        gt_found = gt_chunk is not None and gt_chunk.has_entries()\n",
    "        hyp_found = hyp_chunk is not None and hyp_chunk.has_entries()\n",
    "        \n",
    "        gt_entries = gt_chunk.get_entries() if gt_chunk else []\n",
    "        hyp_entries = hyp_chunk.get_entries() if hyp_chunk else []\n",
    "        \n",
    "        if not gt_found and not hyp_found:\n",
    "            # Both empty - skip\n",
    "            continue\n",
    "        \n",
    "        if not gt_found:\n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=None,\n",
    "                ground_truth_found=False,\n",
    "                output_found=hyp_found,\n",
    "                gt_entry_count=0,\n",
    "                hyp_entry_count=len(hyp_entries),\n",
    "                error=\"No ground truth for chunk\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        if not hyp_found:\n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=False,\n",
    "                gt_entry_count=len(gt_entries),\n",
    "                hyp_entry_count=0,\n",
    "                error=\"No model output for chunk\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        # Compute metrics for this chunk\n",
    "        try:\n",
    "            gt_data = {\"entries\": gt_entries}\n",
    "            hyp_data = {\"entries\": hyp_entries}\n",
    "            \n",
    "            metrics = compute_extraction_metrics(\n",
    "                ground_truth=gt_data,\n",
    "                hypothesis=hyp_data,\n",
    "                fields_to_evaluate=fields if fields else None,\n",
    "                threshold=threshold,\n",
    "                case_sensitive=case_sensitive,\n",
    "                normalize_ws=normalize_ws,\n",
    "            )\n",
    "            \n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=metrics,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "                gt_entry_count=len(gt_entries),\n",
    "                hyp_entry_count=len(hyp_entries),\n",
    "            ))\n",
    "            valid_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "                gt_entry_count=len(gt_entries),\n",
    "                hyp_entry_count=len(hyp_entries),\n",
    "                error=str(e),\n",
    "            ))\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    aggregated = aggregate_metrics(valid_metrics) if valid_metrics else None\n",
    "    \n",
    "    return SourceEvaluationResult(\n",
    "        category=category_name,\n",
    "        model_name=model_name,\n",
    "        source_name=source_name,\n",
    "        chunk_results=chunk_results,\n",
    "        aggregated_metrics=aggregated,\n",
    "        ground_truth_found=True,\n",
    "        output_found=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Chunk-level evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This section executes the evaluation across the full grid of configured models and categories.\n",
    "\n",
    "### What happens in the code cell\n",
    "\n",
    "1. For each (model, category) pair, the notebook iterates over all discovered sources in that category.\n",
    "2. For each source, it calls evaluate_source_chunks(...) to compute:\n",
    "   - per-chunk results (including errors and missing-data flags)\n",
    "   - per-source aggregated metrics (when available)\n",
    "3. It aggregates all valid chunk metrics within the (model, category) pair into a single ExtractionMetrics object and stores:\n",
    "   - all_results[model_name][cat_name]: the detailed per-source results (for optional inspection)\n",
    "   - all_metrics[model_name][cat_name]: the aggregated metrics used for publication tables and exports\n",
    "\n",
    "### Output\n",
    "\n",
    "During execution, the notebook prints one summary line per evaluated (model, category) with:\n",
    "- Entry-level F1 (Entry F1)\n",
    "- Field micro-averaged F1 (Micro F1)\n",
    "- Counts of evaluated sources and chunks\n",
    "\n",
    "These printed summaries are intended for quick sanity checking; the canonical outputs are the tables and exported files produced later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_category(\n",
    "    category: dict,\n",
    "    model: dict,\n",
    ") -> Tuple[List[SourceEvaluationResult], Optional[ExtractionMetrics]]:\n",
    "    \"\"\"\n",
    "    Evaluate all sources in a category for a given model.\n",
    "    \n",
    "    Args:\n",
    "        category: Category config dict\n",
    "        model: Model config dict\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of per-source results, aggregated metrics)\n",
    "    \"\"\"\n",
    "    cat_name = category[\"name\"]\n",
    "    schema_name = category[\"schema\"]\n",
    "    model_name = model[\"name\"]\n",
    "    \n",
    "    sources = discover_sources(cat_name)\n",
    "    results = []\n",
    "    all_chunk_metrics = []\n",
    "    \n",
    "    for source in sources:\n",
    "        result = evaluate_source_chunks(cat_name, model_name, source, schema_name)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Collect valid chunk metrics for aggregation\n",
    "        for chunk_result in result.chunk_results:\n",
    "            if chunk_result.metrics is not None:\n",
    "                all_chunk_metrics.append(chunk_result.metrics)\n",
    "    \n",
    "    aggregated = aggregate_metrics(all_chunk_metrics) if all_chunk_metrics else None\n",
    "    \n",
    "    return results, aggregated\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "all_metrics = {}\n",
    "all_results = {}  # Store detailed results for reporting\n",
    "\n",
    "for model in models:\n",
    "    model_name = model[\"name\"]\n",
    "    all_metrics[model_name] = {}\n",
    "    all_results[model_name] = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_name = category[\"name\"]\n",
    "        \n",
    "        results, aggregated = evaluate_category(category, model)\n",
    "        all_results[model_name][cat_name] = results\n",
    "        \n",
    "        if aggregated and aggregated.total_gt_entries > 0:\n",
    "            all_metrics[model_name][cat_name] = aggregated\n",
    "            evaluated_sources = sum(1 for r in results if r.aggregated_metrics is not None)\n",
    "            total_chunks = sum(r.total_chunks for r in results)\n",
    "            print(f\"{model_name} / {cat_name}: \"\n",
    "                  f\"Entry F1={aggregated.entry_f1:.2%}, \"\n",
    "                  f\"Micro F1={aggregated.micro_f1:.2%} \"\n",
    "                  f\"({evaluated_sources} sources, {total_chunks} chunks)\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table\n",
    "\n",
    "This section produces the main paper-ready summary of extraction quality by model and document category.\n",
    "\n",
    "### Table contents\n",
    "\n",
    "For each (model, category) combination with available metrics, the table reports:\n",
    "\n",
    "- Entry-level metrics:\n",
    "  - Entry P (%), Entry R (%), Entry F1 (%)\n",
    "- Field-level micro-averaged metrics:\n",
    "  - Micro P (%), Micro R (%), Micro F1 (%)\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- The table is displayed as HTML in the notebook for readability and copy/paste workflows.\n",
    "- If SAVE_TABLES_LATEX is True, the same table is exported as:\n",
    "  - OUTPUT_DIR/table_1_extraction_summary.tex\n",
    "\n",
    "This table is the primary high-level result suitable for inclusion in a social science paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Results Summary Table\n",
    "# ================================================================\n",
    "\n",
    "def metrics_to_summary_dataframe(\n",
    "    model_metrics: Dict[str, Dict[str, ExtractionMetrics]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert model metrics dictionary to a pandas DataFrame for display and export.\n",
    "\n",
    "    Args:\n",
    "        model_metrics: Dict mapping model_name -> category -> ExtractionMetrics\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one row per model/category combination\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model_name in sorted(model_metrics.keys()):\n",
    "        for cat_name, m in model_metrics[model_name].items():\n",
    "            rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Category\": cat_name,\n",
    "                \"Entry P (%)\": round(m.entry_precision * 100, 2),\n",
    "                \"Entry R (%)\": round(m.entry_recall * 100, 2),\n",
    "                \"Entry F1 (%)\": round(m.entry_f1 * 100, 2),\n",
    "                \"Micro P (%)\": round(m.micro_precision * 100, 2),\n",
    "                \"Micro R (%)\": round(m.micro_recall * 100, 2),\n",
    "                \"Micro F1 (%)\": round(m.micro_f1 * 100, 2),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "if all_metrics:\n",
    "    # Convert to DataFrame\n",
    "    df_summary = metrics_to_summary_dataframe(all_metrics)\n",
    "\n",
    "    # Display as HTML\n",
    "    display(HTML(\"<h3>Table 1: Extraction Quality Summary</h3>\"))\n",
    "    display(HTML(df_summary.to_html(index=False)))\n",
    "\n",
    "    # Save as LaTeX if configured\n",
    "    if SAVE_TABLES_LATEX:\n",
    "        latex_path = OUTPUT_DIR / \"table_1_extraction_summary.tex\"\n",
    "        df_summary.to_latex(\n",
    "            latex_path,\n",
    "            index=False,\n",
    "            caption=\"Extraction Quality by Model and Category\",\n",
    "            label=\"tab:extraction_summary\",\n",
    "            float_format=\"%.2f\",\n",
    "        )\n",
    "        print(f\"Saved: {latex_path}\")\n",
    "else:\n",
    "    print(\"No metrics computed. Check that ground truth and model outputs exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field-Level Breakdown\n",
    "\n",
    "This section provides a detailed, field-by-field view of extraction performance for each evaluated (model, category) pair.\n",
    "\n",
    "### Table contents\n",
    "\n",
    "For each extracted field, the table reports:\n",
    "\n",
    "- Precision (%), Recall (%), F1 (%)\n",
    "- TP, FP, FN counts (true positives, false positives, false negatives)\n",
    "\n",
    "This breakdown is useful for:\n",
    "- identifying which fields drive overall performance differences\n",
    "- diagnosing systematic failure modes (e.g., consistently low recall on specific attributes)\n",
    "\n",
    "### Outputs\n",
    "\n",
    "For each (model, category) pair:\n",
    "\n",
    "- The field table is rendered as HTML in the notebook.\n",
    "- If SAVE_TABLES_LATEX is True, a corresponding LaTeX table is written to OUTPUT_DIR with a filename that encodes the table number, model, and category (to avoid collisions).\n",
    "\n",
    "Because these tables can be numerous (one per model-category pair), they are designed to be machine-exportable while still remaining readable in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Field-Level Breakdown\n",
    "# ================================================================\n",
    "\n",
    "def field_metrics_to_dataframe(metrics: ExtractionMetrics) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert field-level metrics to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        metrics: ExtractionMetrics object\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one row per field\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for field_name, fm in sorted(metrics.field_metrics.items()):\n",
    "        rows.append({\n",
    "            \"Field\": field_name,\n",
    "            \"Precision (%)\": round(fm.precision * 100, 2),\n",
    "            \"Recall (%)\": round(fm.recall * 100, 2),\n",
    "            \"F1 (%)\": round(fm.f1 * 100, 2),\n",
    "            \"TP\": fm.true_positives,\n",
    "            \"FP\": fm.false_positives,\n",
    "            \"FN\": fm.false_negatives,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Display field-level breakdown for each model/category\n",
    "table_counter = 2  # Start after Table 1\n",
    "\n",
    "for model_name, cat_metrics in all_metrics.items():\n",
    "    for cat_name, metrics in cat_metrics.items():\n",
    "        # Convert to DataFrame\n",
    "        df_fields = field_metrics_to_dataframe(metrics)\n",
    "\n",
    "        # Display as HTML\n",
    "        display(HTML(f\"<h4>Table {table_counter}: Field-Level Metrics â€” {model_name} / {cat_name}</h4>\"))\n",
    "        display(HTML(df_fields.to_html(index=False)))\n",
    "\n",
    "        # Save as LaTeX if configured\n",
    "        if SAVE_TABLES_LATEX:\n",
    "            # Create safe filename\n",
    "            safe_model = model_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            safe_cat = cat_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            latex_path = OUTPUT_DIR / f\"table_{table_counter}_fields_{safe_model}_{safe_cat}.tex\"\n",
    "            df_fields.to_latex(\n",
    "                latex_path,\n",
    "                index=False,\n",
    "                caption=f\"Field-Level Metrics for {model_name} on {cat_name}\",\n",
    "                label=f\"tab:fields_{safe_model}_{safe_cat}\",\n",
    "                float_format=\"%.2f\",\n",
    "            )\n",
    "            print(f\"Saved: {latex_path}\")\n",
    "\n",
    "        table_counter += 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Source Details (Optional)\n",
    "\n",
    "This section is intentionally optional and is meant for targeted inspection during development or analysis.\n",
    "\n",
    "The code cell below contains a commented snippet that can be enabled to print per-source summaries for a selected:\n",
    "\n",
    "- SHOW_MODEL\n",
    "- SHOW_CATEGORY\n",
    "\n",
    "This helps identify whether a model's aggregate performance is driven by a small number of difficult sources or is consistent across sources.\n",
    "\n",
    "No outputs are produced unless you uncomment and run the snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-source breakdown for a specific model/category\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# SHOW_MODEL = \"gpt_5.1_medium\"\n",
    "# SHOW_CATEGORY = \"bibliography\"\n",
    "\n",
    "# if SHOW_MODEL in all_results and SHOW_CATEGORY in all_results[SHOW_MODEL]:\n",
    "#     results = all_results[SHOW_MODEL][SHOW_CATEGORY]\n",
    "#     print(f\"\\n{SHOW_MODEL} / {SHOW_CATEGORY}:\")\n",
    "#     print(\"-\" * 60)\n",
    "#     for r in results:\n",
    "#         if r.aggregated_metrics:\n",
    "#             m = r.aggregated_metrics\n",
    "#             print(f\"  {r.source_name}: F1={m.entry_f1:.2%}, \"\n",
    "#                   f\"{r.evaluated_chunks}/{r.total_chunks} chunks\")\n",
    "#         else:\n",
    "#             print(f\"  {r.source_name}: {r.error or 'No data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Reports\n",
    "\n",
    "This section exports evaluation results in formats suitable for reproducible research workflows and downstream analysis.\n",
    "\n",
    "### Files written\n",
    "\n",
    "The code cell writes:\n",
    "\n",
    "- JSON (eval_results_{timestamp}.json)\n",
    "  A structured, machine-readable report containing:\n",
    "  - run timestamp\n",
    "  - evaluation parameter settings (threshold, case_sensitive, normalize_whitespace)\n",
    "  - aggregated metrics serialized via ExtractionMetrics.to_dict()\n",
    "\n",
    "- CSV (eval_results_{timestamp}.csv)\n",
    "  A flat summary table (matching the Results Summary Table section) for easy import into:\n",
    "  - statistical software\n",
    "  - spreadsheets\n",
    "  - manuscript workflows\n",
    "\n",
    "Additionally, if SAVE_TABLES_LATEX is True, the section prints a count of .tex files present in OUTPUT_DIR, which serves as a quick validation that LaTeX exports were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Save Reports (JSON, CSV)\n",
    "# ================================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save JSON Report ---\n",
    "json_path = reports_path / f\"eval_results_{timestamp}.json\"\n",
    "json_data = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"evaluation_method\": \"chunk-level\",\n",
    "    \"config\": {\n",
    "        \"threshold\": threshold,\n",
    "        \"case_sensitive\": case_sensitive,\n",
    "        \"normalize_whitespace\": normalize_ws,\n",
    "    },\n",
    "    \"results\": {\n",
    "        model: {cat: m.to_dict() for cat, m in cats.items()}\n",
    "        for model, cats in all_metrics.items()\n",
    "    },\n",
    "}\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Saved JSON report: {json_path}\")\n",
    "\n",
    "# --- Save CSV Report ---\n",
    "csv_path = reports_path / f\"eval_results_{timestamp}.csv\"\n",
    "if all_metrics:\n",
    "    df_summary = metrics_to_summary_dataframe(all_metrics)\n",
    "    df_summary.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved CSV report: {csv_path}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"REPORTS SAVED\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  JSON: {json_path.name}\")\n",
    "print(f\"  CSV:  {csv_path.name}\")\n",
    "if SAVE_TABLES_LATEX:\n",
    "    tex_files = list(OUTPUT_DIR.glob(\"*.tex\"))\n",
    "    print(f\"  LaTeX tables: {len(tex_files)} files in {OUTPUT_DIR.relative_to(EVAL_DIR)}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Optional)\n",
    "\n",
    "This section produces a compact visual summary of model performance across categories.\n",
    "\n",
    "### Figure\n",
    "\n",
    "If metrics are available, the code creates a grouped bar chart:\n",
    "\n",
    "- x-axis: model\n",
    "- bars: category\n",
    "- y-axis: micro-averaged F1 (Micro F1 (%))\n",
    "\n",
    "### Files written\n",
    "\n",
    "- A PNG figure is saved to reports_path as eval_chart_{timestamp}.png.\n",
    "- If SAVE_TABLES_LATEX is True, a PDF copy is also saved to OUTPUT_DIR/figure_extraction_quality.pdf for LaTeX inclusion.\n",
    "\n",
    "If matplotlib is not installed, the section is skipped gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Visualization\n",
    "# ================================================================\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\"No metrics to visualize\")\n",
    "    else:\n",
    "        # Prepare data for plotting\n",
    "        model_names = list(all_metrics.keys())\n",
    "        cat_names = list(set(\n",
    "            cat for cats in all_metrics.values() for cat in cats.keys()\n",
    "        ))\n",
    "\n",
    "        if model_names and cat_names:\n",
    "            # Create grouped bar chart for F1 scores\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            x = np.arange(len(model_names))\n",
    "            width = 0.8 / len(cat_names)\n",
    "\n",
    "            for i, cat in enumerate(cat_names):\n",
    "                f1_scores = [\n",
    "                    all_metrics[model].get(cat, ExtractionMetrics()).micro_f1 * 100\n",
    "                    for model in model_names\n",
    "                ]\n",
    "                offset = (i - len(cat_names) / 2 + 0.5) * width\n",
    "                ax.bar(x + offset, f1_scores, width, label=cat)\n",
    "\n",
    "            ax.set_ylabel(\"Micro F1 Score (%)\")\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_title(\"Extraction Quality by Model and Category (Chunk-Level Evaluation)\")\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "            ax.legend(title=\"Category\")\n",
    "            ax.set_ylim(0, 100)\n",
    "            ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save chart\n",
    "            chart_path = reports_path / f\"eval_chart_{timestamp}.png\"\n",
    "            plt.savefig(chart_path, dpi=150)\n",
    "            print(f\"Saved chart: {chart_path}\")\n",
    "\n",
    "            # Save as PDF for LaTeX inclusion\n",
    "            if SAVE_TABLES_LATEX:\n",
    "                pdf_path = OUTPUT_DIR / \"figure_extraction_quality.pdf\"\n",
    "                plt.savefig(pdf_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "                print(f\"Saved PDF figure: {pdf_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No data to visualize\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"matplotlib not available - skipping visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
