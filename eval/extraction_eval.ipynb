{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoMiner Extraction Evaluation\n",
    "\n",
    "This notebook evaluates structured data extraction quality across multiple LLM providers and models.\n",
    "\n",
    "## Evaluation Method\n",
    "\n",
    "Metrics are computed **chunk-by-chunk** using the temporary JSONL files produced by the extractor.\n",
    "This approach:\n",
    "- **Eliminates formatting penalties** from whitespace differences in final JSON output\n",
    "- **Enables accurate per-chunk error attribution** for debugging\n",
    "- **Isolates extraction quality** from post-processing effects\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure you have:\n",
    "1. Corrected transcription files in `test_data/input/{category}/`\n",
    "2. Ground truth JSONL files in `test_data/ground_truth/{category}/`\n",
    "3. Model output JSONL files in `test_data/output/{category}/{model_name}/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Add parent directory for imports\n",
    "EVAL_DIR = Path.cwd()\n",
    "PROJECT_ROOT = EVAL_DIR.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "\n",
    "# Import extraction metrics\n",
    "from metrics import (\n",
    "    ExtractionMetrics,\n",
    "    aggregate_metrics,\n",
    "    compute_extraction_metrics,\n",
    "    format_field_metrics_table,\n",
    "    format_metrics_table,\n",
    ")\n",
    "\n",
    "# Import JSONL chunk-level utilities\n",
    "from jsonl_eval import (\n",
    "    ChunkExtraction,\n",
    "    DocumentExtractions,\n",
    "    parse_extraction_jsonl,\n",
    "    find_jsonl_file,\n",
    "    load_chunk_extractions,\n",
    "    load_ground_truth_chunks,\n",
    "    align_chunks,\n",
    ")\n",
    "\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation config\n",
    "config_path = EVAL_DIR / \"eval_config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract paths\n",
    "input_path = EVAL_DIR / config[\"dataset\"][\"input_path\"]\n",
    "output_path = EVAL_DIR / config[\"dataset\"][\"output_path\"]\n",
    "ground_truth_path = EVAL_DIR / config[\"dataset\"][\"ground_truth_path\"]\n",
    "reports_path = EVAL_DIR / config[\"evaluation\"][\"reports_path\"]\n",
    "\n",
    "# Evaluation settings\n",
    "threshold = config[\"evaluation\"].get(\"string_similarity_threshold\", 0.85)\n",
    "case_sensitive = config[\"evaluation\"].get(\"case_sensitive\", False)\n",
    "normalize_ws = config[\"evaluation\"].get(\"normalize_whitespace\", True)\n",
    "schema_fields = config[\"evaluation\"].get(\"schema_fields\", {})\n",
    "\n",
    "categories = config[\"dataset\"][\"categories\"]\n",
    "models = config[\"models\"]\n",
    "\n",
    "# Create reports directory\n",
    "reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Categories: {[c['name'] for c in categories]}\")\n",
    "print(f\"Models: {[m['name'] for m in models]}\")\n",
    "print(f\"Similarity threshold: {threshold}\")\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Input: {input_path}\")\n",
    "print(f\"  Output: {output_path}\")\n",
    "print(f\"  Ground Truth: {ground_truth_path}\")\n",
    "print(f\"  Reports: {reports_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_sources(category_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover source files in the input directory for a category.\n",
    "    \n",
    "    Returns:\n",
    "        List of source names (without extension)\n",
    "    \"\"\"\n",
    "    input_dir = input_path / category_name\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    sources = []\n",
    "    for input_file in sorted(input_dir.glob(\"*.txt\")):\n",
    "        stem = input_file.stem\n",
    "        if stem.endswith(\"_line_ranges\") or stem.endswith(\"_context\"):\n",
    "            continue\n",
    "        sources.append(stem)\n",
    "    \n",
    "    return sources\n",
    "\n",
    "\n",
    "def discover_available_models(category_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover which models have JSONL output for a category.\n",
    "    \n",
    "    Returns:\n",
    "        List of model names with available output\n",
    "    \"\"\"\n",
    "    cat_output = output_path / category_name\n",
    "    \n",
    "    if not cat_output.exists():\n",
    "        return []\n",
    "    \n",
    "    available = []\n",
    "    for d in cat_output.iterdir():\n",
    "        if d.is_dir():\n",
    "            # Check if model directory has any JSONL files\n",
    "            jsonl_files = list(d.rglob(\"*.jsonl\"))\n",
    "            # Filter out batch tracking files\n",
    "            jsonl_files = [f for f in jsonl_files if \"_batch_\" not in f.name]\n",
    "            if jsonl_files:\n",
    "                available.append(d.name)\n",
    "    \n",
    "    return sorted(available)\n",
    "\n",
    "\n",
    "def check_ground_truth_available(category_name: str) -> Tuple[bool, int, str]:\n",
    "    \"\"\"\n",
    "    Check if ground truth files exist for a category.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (has_ground_truth, count_of_files, format)\n",
    "    \"\"\"\n",
    "    gt_dir = ground_truth_path / category_name\n",
    "    if not gt_dir.exists():\n",
    "        return False, 0, \"none\"\n",
    "    \n",
    "    # Check for JSONL format (preferred)\n",
    "    jsonl_files = list(gt_dir.glob(\"*.jsonl\"))\n",
    "    if jsonl_files:\n",
    "        return True, len(jsonl_files), \"jsonl\"\n",
    "    \n",
    "    # Fall back to JSON format (legacy)\n",
    "    json_files = list(gt_dir.glob(\"*.json\"))\n",
    "    if json_files:\n",
    "        return True, len(json_files), \"json\"\n",
    "    \n",
    "    return False, 0, \"none\"\n",
    "\n",
    "\n",
    "# Discover and display available data\n",
    "print(\"=\" * 60)\n",
    "print(\"AVAILABLE DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_summary = {}\n",
    "\n",
    "for cat in categories:\n",
    "    cat_name = cat[\"name\"]\n",
    "    sources = discover_sources(cat_name)\n",
    "    available_models = discover_available_models(cat_name)\n",
    "    gt_available, gt_count, gt_format = check_ground_truth_available(cat_name)\n",
    "    \n",
    "    data_summary[cat_name] = {\n",
    "        \"sources\": sources,\n",
    "        \"models\": available_models,\n",
    "        \"ground_truth_available\": gt_available,\n",
    "        \"ground_truth_count\": gt_count,\n",
    "        \"ground_truth_format\": gt_format,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{cat_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Input sources: {len(sources)}\")\n",
    "    if sources:\n",
    "        for s in sources[:5]:\n",
    "            print(f\"    - {s}\")\n",
    "        if len(sources) > 5:\n",
    "            print(f\"    ... and {len(sources) - 5} more\")\n",
    "    print(f\"  Models with JSONL output: {len(available_models)}\")\n",
    "    for m in available_models:\n",
    "        print(f\"    - {m}\")\n",
    "    print(f\"  Ground truth: {'Yes' if gt_available else 'No'} ({gt_count} files, format: {gt_format})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk-Level Evaluation\n",
    "\n",
    "Metrics are computed by comparing each chunk of the model output against the corresponding\n",
    "ground truth chunk. This ensures:\n",
    "- No formatting penalties from whitespace differences in final JSON output\n",
    "- Accurate per-chunk error attribution\n",
    "- Better isolation of extraction quality from post-processing effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkEvaluationResult:\n",
    "    \"\"\"Container for per-chunk evaluation results.\"\"\"\n",
    "    chunk_index: int\n",
    "    custom_id: str\n",
    "    metrics: Optional[ExtractionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    gt_entry_count: int = 0\n",
    "    hyp_entry_count: int = 0\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SourceEvaluationResult:\n",
    "    \"\"\"Container for source-level evaluation results.\"\"\"\n",
    "    category: str\n",
    "    model_name: str\n",
    "    source_name: str\n",
    "    chunk_results: List[ChunkEvaluationResult]\n",
    "    aggregated_metrics: Optional[ExtractionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def total_chunks(self) -> int:\n",
    "        return len(self.chunk_results)\n",
    "    \n",
    "    @property\n",
    "    def evaluated_chunks(self) -> int:\n",
    "        return sum(1 for c in self.chunk_results if c.metrics is not None)\n",
    "\n",
    "\n",
    "def evaluate_source_chunks(\n",
    "    category_name: str,\n",
    "    model_name: str,\n",
    "    source_name: str,\n",
    "    schema_name: str,\n",
    ") -> SourceEvaluationResult:\n",
    "    \"\"\"\n",
    "    Evaluate a source by comparing chunks from model output to ground truth.\n",
    "    \n",
    "    Args:\n",
    "        category_name: Dataset category\n",
    "        model_name: Model identifier\n",
    "        source_name: Source file name\n",
    "        schema_name: Schema name for field selection\n",
    "        \n",
    "    Returns:\n",
    "        SourceEvaluationResult with per-chunk and aggregated metrics\n",
    "    \"\"\"\n",
    "    # Load ground truth chunks\n",
    "    gt_doc = load_ground_truth_chunks(ground_truth_path, category_name, source_name)\n",
    "    if gt_doc is None or not gt_doc.chunks:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category_name,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            chunk_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=False,\n",
    "            output_found=False,\n",
    "            error=\"Ground truth not found\",\n",
    "        )\n",
    "    \n",
    "    # Load model output chunks\n",
    "    hyp_doc = load_chunk_extractions(output_path, category_name, model_name, source_name)\n",
    "    if hyp_doc is None or not hyp_doc.chunks:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category_name,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            chunk_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=True,\n",
    "            output_found=False,\n",
    "            error=\"Model output not found\",\n",
    "        )\n",
    "    \n",
    "    # Get fields to evaluate for this schema\n",
    "    fields = schema_fields.get(schema_name, [])\n",
    "    \n",
    "    # Align chunks\n",
    "    aligned = align_chunks(hyp_doc, gt_doc)\n",
    "    \n",
    "    # Compute per-chunk metrics\n",
    "    chunk_results: List[ChunkEvaluationResult] = []\n",
    "    valid_metrics: List[ExtractionMetrics] = []\n",
    "    \n",
    "    for hyp_chunk, gt_chunk in aligned:\n",
    "        # Determine chunk info\n",
    "        if gt_chunk:\n",
    "            chunk_index = gt_chunk.chunk_index\n",
    "            custom_id = gt_chunk.custom_id or (hyp_chunk.custom_id if hyp_chunk else \"\")\n",
    "        elif hyp_chunk:\n",
    "            chunk_index = hyp_chunk.chunk_index\n",
    "            custom_id = hyp_chunk.custom_id\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Check availability\n",
    "        gt_found = gt_chunk is not None and gt_chunk.has_entries()\n",
    "        hyp_found = hyp_chunk is not None and hyp_chunk.has_entries()\n",
    "        \n",
    "        gt_entries = gt_chunk.get_entries() if gt_chunk else []\n",
    "        hyp_entries = hyp_chunk.get_entries() if hyp_chunk else []\n",
    "        \n",
    "        if not gt_found and not hyp_found:\n",
    "            # Both empty - skip\n",
    "            continue\n",
    "        \n",
    "        if not gt_found:\n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=None,\n",
    "                ground_truth_found=False,\n",
    "                output_found=hyp_found,\n",
    "                gt_entry_count=0,\n",
    "                hyp_entry_count=len(hyp_entries),\n",
    "                error=\"No ground truth for chunk\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        if not hyp_found:\n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=False,\n",
    "                gt_entry_count=len(gt_entries),\n",
    "                hyp_entry_count=0,\n",
    "                error=\"No model output for chunk\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        # Compute metrics for this chunk\n",
    "        try:\n",
    "            gt_data = {\"entries\": gt_entries}\n",
    "            hyp_data = {\"entries\": hyp_entries}\n",
    "            \n",
    "            metrics = compute_extraction_metrics(\n",
    "                ground_truth=gt_data,\n",
    "                hypothesis=hyp_data,\n",
    "                fields_to_evaluate=fields if fields else None,\n",
    "                threshold=threshold,\n",
    "                case_sensitive=case_sensitive,\n",
    "                normalize_ws=normalize_ws,\n",
    "            )\n",
    "            \n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=metrics,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "                gt_entry_count=len(gt_entries),\n",
    "                hyp_entry_count=len(hyp_entries),\n",
    "            ))\n",
    "            valid_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            chunk_results.append(ChunkEvaluationResult(\n",
    "                chunk_index=chunk_index,\n",
    "                custom_id=custom_id,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "                gt_entry_count=len(gt_entries),\n",
    "                hyp_entry_count=len(hyp_entries),\n",
    "                error=str(e),\n",
    "            ))\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    aggregated = aggregate_metrics(valid_metrics) if valid_metrics else None\n",
    "    \n",
    "    return SourceEvaluationResult(\n",
    "        category=category_name,\n",
    "        model_name=model_name,\n",
    "        source_name=source_name,\n",
    "        chunk_results=chunk_results,\n",
    "        aggregated_metrics=aggregated,\n",
    "        ground_truth_found=True,\n",
    "        output_found=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Chunk-level evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_category(\n",
    "    category: dict,\n",
    "    model: dict,\n",
    ") -> Tuple[List[SourceEvaluationResult], Optional[ExtractionMetrics]]:\n",
    "    \"\"\"\n",
    "    Evaluate all sources in a category for a given model.\n",
    "    \n",
    "    Args:\n",
    "        category: Category config dict\n",
    "        model: Model config dict\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of per-source results, aggregated metrics)\n",
    "    \"\"\"\n",
    "    cat_name = category[\"name\"]\n",
    "    schema_name = category[\"schema\"]\n",
    "    model_name = model[\"name\"]\n",
    "    \n",
    "    sources = discover_sources(cat_name)\n",
    "    results = []\n",
    "    all_chunk_metrics = []\n",
    "    \n",
    "    for source in sources:\n",
    "        result = evaluate_source_chunks(cat_name, model_name, source, schema_name)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Collect valid chunk metrics for aggregation\n",
    "        for chunk_result in result.chunk_results:\n",
    "            if chunk_result.metrics is not None:\n",
    "                all_chunk_metrics.append(chunk_result.metrics)\n",
    "    \n",
    "    aggregated = aggregate_metrics(all_chunk_metrics) if all_chunk_metrics else None\n",
    "    \n",
    "    return results, aggregated\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "all_metrics = {}\n",
    "all_results = {}  # Store detailed results for reporting\n",
    "\n",
    "for model in models:\n",
    "    model_name = model[\"name\"]\n",
    "    all_metrics[model_name] = {}\n",
    "    all_results[model_name] = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_name = category[\"name\"]\n",
    "        \n",
    "        results, aggregated = evaluate_category(category, model)\n",
    "        all_results[model_name][cat_name] = results\n",
    "        \n",
    "        if aggregated and aggregated.total_gt_entries > 0:\n",
    "            all_metrics[model_name][cat_name] = aggregated\n",
    "            evaluated_sources = sum(1 for r in results if r.aggregated_metrics is not None)\n",
    "            total_chunks = sum(r.total_chunks for r in results)\n",
    "            print(f\"{model_name} / {cat_name}: \"\n",
    "                  f\"Entry F1={aggregated.entry_f1:.2%}, \"\n",
    "                  f\"Micro F1={aggregated.micro_f1:.2%} \"\n",
    "                  f\"({evaluated_sources} sources, {total_chunks} chunks)\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Display summary table\n",
    "if all_metrics:\n",
    "    summary_table = format_metrics_table(all_metrics)\n",
    "    display(Markdown(summary_table))\n",
    "else:\n",
    "    print(\"No metrics computed. Check that ground truth and model outputs exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field-Level Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show field-level breakdown for each model/category\n",
    "for model_name, cat_metrics in all_metrics.items():\n",
    "    for cat_name, metrics in cat_metrics.items():\n",
    "        display(Markdown(f\"### {model_name} / {cat_name}\"))\n",
    "        display(Markdown(format_field_metrics_table(metrics)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Source Details (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-source breakdown for a specific model/category\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# SHOW_MODEL = \"gpt_5.1_medium\"\n",
    "# SHOW_CATEGORY = \"bibliography\"\n",
    "\n",
    "# if SHOW_MODEL in all_results and SHOW_CATEGORY in all_results[SHOW_MODEL]:\n",
    "#     results = all_results[SHOW_MODEL][SHOW_CATEGORY]\n",
    "#     print(f\"\\n{SHOW_MODEL} / {SHOW_CATEGORY}:\")\n",
    "#     print(\"-\" * 60)\n",
    "#     for r in results:\n",
    "#         if r.aggregated_metrics:\n",
    "#             m = r.aggregated_metrics\n",
    "#             print(f\"  {r.source_name}: F1={m.entry_f1:.2%}, \"\n",
    "#                   f\"{r.evaluated_chunks}/{r.total_chunks} chunks\")\n",
    "#         else:\n",
    "#             print(f\"  {r.source_name}: {r.error or 'No data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON\n",
    "json_path = reports_path / f\"eval_results_{timestamp}.json\"\n",
    "json_data = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"evaluation_method\": \"chunk-level\",\n",
    "    \"config\": {\n",
    "        \"threshold\": threshold,\n",
    "        \"case_sensitive\": case_sensitive,\n",
    "        \"normalize_whitespace\": normalize_ws,\n",
    "    },\n",
    "    \"results\": {\n",
    "        model: {cat: m.to_dict() for cat, m in cats.items()}\n",
    "        for model, cats in all_metrics.items()\n",
    "    },\n",
    "}\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Saved: {json_path}\")\n",
    "\n",
    "# Save CSV\n",
    "csv_path = reports_path / f\"eval_results_{timestamp}.csv\"\n",
    "rows = []\n",
    "for model_name, cat_metrics in all_metrics.items():\n",
    "    for cat_name, m in cat_metrics.items():\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"category\": cat_name,\n",
    "            \"entry_precision\": round(m.entry_precision * 100, 2),\n",
    "            \"entry_recall\": round(m.entry_recall * 100, 2),\n",
    "            \"entry_f1\": round(m.entry_f1 * 100, 2),\n",
    "            \"micro_precision\": round(m.micro_precision * 100, 2),\n",
    "            \"micro_recall\": round(m.micro_recall * 100, 2),\n",
    "            \"micro_f1\": round(m.micro_f1 * 100, 2),\n",
    "        })\n",
    "if rows:\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "\n",
    "# Save Markdown\n",
    "md_path = reports_path / f\"eval_results_{timestamp}.md\"\n",
    "md_content = f\"\"\"# ChronoMiner Extraction Evaluation Results\n",
    "\n",
    "**Generated:** {timestamp}\n",
    "\n",
    "**Evaluation Method:** Chunk-level (using temporary JSONL files)\n",
    "\n",
    "## Summary\n",
    "\n",
    "{format_metrics_table(all_metrics) if all_metrics else 'No results'}\n",
    "\"\"\"\n",
    "with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md_content)\n",
    "print(f\"Saved: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    if not all_metrics:\n",
    "        print(\"No metrics to visualize\")\n",
    "    else:\n",
    "        # Prepare data for plotting\n",
    "        model_names = list(all_metrics.keys())\n",
    "        cat_names = list(set(\n",
    "            cat for cats in all_metrics.values() for cat in cats.keys()\n",
    "        ))\n",
    "        \n",
    "        if model_names and cat_names:\n",
    "            # Create grouped bar chart for F1 scores\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            x = np.arange(len(model_names))\n",
    "            width = 0.8 / len(cat_names)\n",
    "            \n",
    "            for i, cat in enumerate(cat_names):\n",
    "                f1_scores = [\n",
    "                    all_metrics[model].get(cat, ExtractionMetrics()).micro_f1 * 100\n",
    "                    for model in model_names\n",
    "                ]\n",
    "                offset = (i - len(cat_names) / 2 + 0.5) * width\n",
    "                ax.bar(x + offset, f1_scores, width, label=cat)\n",
    "            \n",
    "            ax.set_ylabel(\"Micro F1 Score (%)\")\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_title(\"Extraction Quality by Model and Category (Chunk-Level Evaluation)\")\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "            ax.legend(title=\"Category\")\n",
    "            ax.set_ylim(0, 100)\n",
    "            ax.grid(axis=\"y\", alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save chart\n",
    "            chart_path = reports_path / f\"eval_chart_{timestamp}.png\"\n",
    "            plt.savefig(chart_path, dpi=150)\n",
    "            print(f\"Saved: {chart_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No data to visualize\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"matplotlib not available - skipping visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}